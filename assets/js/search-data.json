{
  
    
        "post0": {
            "title": "Using Temporary Files In Python",
            "content": "Using Temporary Files in Python . So in most instances when working on a project, you would have access to read and write from the folders which you are working in? But what if you don’t? That’s the dilemma I faced while using a certain data platform where I didn’t have direct access to the underlying folder structure. What to do in this case when you need to write a file then? In particular, I was using matplotlib/plotly to create some visualizations and I needed to put the png/html files somewhere. Temporary files to the rescue! . import tempfile import datetime now = datetime.datetime.now() fd, path = tempfile.mkstemp(suffix=&#39;.png&#39;) try: #Write the info to the temporary file plt.savefig(path) with open(path) as f: #Put the temporary file into the required folder new_folder.write(&quot;Image&quot;+now.strftime(&quot;%Y-%m-%d&quot;)+&quot;.png&quot;, f) finally: #Remove the temporary file os.remove(path) . You can extend this to any file type you want and may prove to be useful whenever you’re dealing with systems where you don’t have direct access to folders but you can read/write to them via an API call. Just replace the pseudocode of new_folder.write() with whatever you need! .",
            "url": "https://bakperi.com/2022/02/13/Using-Temporary-Files-In-Python.html",
            "relUrl": "/2022/02/13/Using-Temporary-Files-In-Python.html",
            "date": " • Feb 13, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Performance Measures",
            "content": "Real World Data Science - Performance Measures . The world of academia or Kaggle competitions can be very different when it comes to deciding what makes a good model. Measures such as AUC, precision, recall and F1 score are used to measure the accuracy of classification models, especially those where there is an imbalance of classes in the training set. . Confusion Matrix (https://en.wikipedia.org/wiki/Confusion_matrix) . One of the best overall metrics for judging the performance of a classification model is the F1 score as it considers both precision and recall equally while handling imbalance better than accuracy. However, there are some instances where precision is more important than recall and vice versa. . Consider the following scenario from a marketing perspective. In a situation where you may be trying to attract new consumers over to your product who haven’t historically purchased your product in the past, then there is some allowance for false positives in a model which determines product purchasers. Even though they might not be a purchaser now, they could become one in the future so it’s still worthwhile to use meaningful campaigns towards them. Whereas with false negatives, the consumers may have purchased historically but because you are now ignoring them from a marketing perspective, you may lose some of that audience. . This is why another, potentially more useful metric than F1 score is to use cost matrix gain. That is, what is the financial impact of classifying a data point as a True Postive, True Negative, False Positive, or False Negative? .",
            "url": "https://bakperi.com/2021/06/23/performance-measures.html",
            "relUrl": "/2021/06/23/performance-measures.html",
            "date": " • Jun 23, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "March Madness 2018",
            "content": "March Madness 2018 Kaggle Competition . Kaggle is a pretty popular website for machine learning competitions and playgrounds that are familiar to most people in the data science world. So among the many contests they have available, one in particular piqued my interest a few weeks ago which was this year’s March Madness bracket competition! Personally, I’m an NBA fan but the excitement around NCAA Division I basketball has always interested me. . It’s surprisingly easy to get started with a competition in Kaggle, and there was a good starter kernel provided by someone from Google to help people get started. I used that in addition to reading some of the discussion threads. . My code for both the men’s and women’s competition is available on my GitHub. For my relatively simple models, I was able to place 113th out of 934 for the men’s and 182nd out of 505 for the women’s. Not too shabby! . So what was the basic idea behind the model? For each team, we build a vector consisting of summary statistics of team’s performance over the season. . By using the data provided by Kaggle, we can build a vector that looks something like this: . return [numWins, avgPoints, avgPointsAllowed, avgFGM, avgOppFGM, avgFGA, avgOppFGA, avgFGM3, avgOppFGM3, avgFGA3, avgOppFGA3, . avgFTM, avgOppFTM, avgFTA, avgOppFTA, avgOR, avgOppOR, avgDR, avgOppDR, avgAst, avgOppAst, avgTO, avgOppTO, avgStl, . avgOppStl, avgBlk, avgOppBlk, avgPF, avgOppPF, tournamentSeed, sos, srs] . So basic basketball stats like points, offensive/defensive rebounds, assists, blocks are given in a team’s vector. We then need to compute the percentage chance that one team will defeat in another by pairing any two teams vectors together. We do this simply by subtracting one team’s vector from another for any particular matchup. I decided to use this method as it was chosen in Google’s example kernel but perhaps other methods such as averaging could be tested in the future. . Using feature importance methods in scikit-learn, I was able to determine that the number of wins in the season was the most predictive followed by advanced analytical scores of teams such as sos, srs, and sag. SOS is the Strength Of Schedule which is a metric designed for determining the strength of opponents a team has faced. SRS is the Simple Rating System which is a metric incorporating SOS and average point differential. SAG is a metric designed by Jeff Sagarin. . . After testing a few classification models in sci-kit learn, I settled on the GradientBoostingClassifier as it provided the most accuracy on historical seasons in Stage 1 of the Kaggle competition. I combined it with GridSearchCV to search the space for the best number of estimators and depth of trees. . Below is the output predictions of my model! In this year’s actual tournament there turned out to be a bunch of upsets including the unprecedented event of a 16th seed upsetting the 1st seed with UMBC upsetting Virginia. Kinda funny how the model predicts them as the winner and they don’t even get out of the first round. Goes to show that in life and basketball there is a lot of variance that is difficult to account for. . . After the competition was over the winner of the women’s competition had an interesting post on his solution (which was partially a fluke): Link . I’m definitely going to try an approach like this for next years contest to test the ability of xgboost! .",
            "url": "https://bakperi.com/2021/04/11/march-madness-2018.html",
            "relUrl": "/2021/04/11/march-madness-2018.html",
            "date": " • Apr 11, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Visit my LinkedIn Profile! .",
          "url": "https://bakperi.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://bakperi.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}